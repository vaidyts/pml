---
title: "Modeling and Prediction using Weight Lifting dataset"
output: html_document
---

### Executive Summary

Under the category of Human Activity Recognition (HAR) is a dataset called Weight Lifting Exercise (WLE). This dataset contains data generated during weight lifting performed by 6 participants, who were asked to lift the weights using different techniques with various sensors attached to their body, the sensor data being the predictors and the weight lifting technique being the response variable. We are given a training and test dataset. The objective of this project is to develop a model based on the provided training data, addressing the questions "how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did", and use the model to predict 20 different test cases. 

The main steps in the above exercise involved reading the data, looking at the summary statistics, selecting the variables for modeling, partitioning the given samples into train and test subsets, model development using the training data, testing the model on the test subset, error estimation, and finally prediction of the response for the sample set provided. Different models were tried out and in the end, the best model turned out to be a random Forest model which outperformed by far the other techniques. The model was able to obtain a near perfect classification of the training and test data, and a perfect classification of the assignment data. Details are given below:

### Data partitioning and variable selection for modeling
The given training data contains 19622 rows (observations) and 160 columns (variables). After reading it in, it is split into a 75/25 partition for training and testing purposes respectively. Such a cross validation approach is needed in order to avoid over-fitting. Summary statistics of each of the variables is collected and written to a log file. It is seen form the log file that 96 of the 160 variables have close to 98% (19216/19620) "NA" or "blank" entries. It is decided to remove these variables from the training. 4 other variables related to serial number and timestamp are also removed from the dataset. This pruning results in a reduced dataset containing 56 variables. 

```{r,include=FALSE}
library(caret); library(tree); library(randomForest); library(rpart)
```
Following lines of code pertain to reading the data.
```{r}
train_given <- read.csv("pml-training.csv")
test_given <- read.csv("pml-testing.csv")
indexTrain <- createDataPartition(y=train_given$classe,p=0.75,list=FALSE)
train <- train_given[indexTrain,]
test <- train_given[-indexTrain,]
```
Following lines of code pertain to the summary statistics of the variables.
```{r, eval=FALSE}
fn <- "summary.txt"
raw <- train_given
for (i in 1:ncol(raw)) { 
  x <- names(raw)[i]
  write(x, file=fn, append=TRUE, sep="\t",ncolumns = 7)
  x <- names(summary(raw[,i], maxsum=7))
  write(x,file=fn, append=TRUE, sep="\t", ncolumns = 7)
  x <- summary(raw[,i], maxsum=7)
  write(x,file=fn, append=TRUE, sep="\t", ncolumns = 7)
  write("\n", file=fn, append=TRUE)
}
```
Folliwng lines of code pertain to discarding those variables which have a majority of blank or NA entries, and setting up the train, test, and validate datasets for subsequent use.
```{r}
varindex <- c(2, 6:11, 37:49, 60:68, 84:86, 102, 113:124, 140, 151:159)
train_x <- train[,varindex]
train_y <- train[,160]
train_xy <- data.frame(train_x,classe=train_y)
test_x <- test[,varindex]
test_y <- test[,160]
test_xy <- data.frame(test_x,classe=test_y)
validate <- test_given[,varindex]
```
### Modeling choices
In this section, different models are attempted. There are numerous modeling algorithms and it is not feasible or practical to try all of them. Of the 2 main approaches - regression vs classification based methods, it was decided to go with classification methods. The reason for this being that the data contains numeric as well as categorical variables, hence classification techniques was used rather than regression techniques. There are many classification algorithms, of which a few were attempted - they are rpart, tree and randomForest. The accuracy kept on increasing ongoing from rpart to tree to randomForest. With randomForest, extremely good accuracy could be obtained, hence it was not necessary to proceed further.

### Model=rpart
The first model tried is rpart which stands for recursive partitioning. The model is run with default options. The resulting accuracy is ~ 0.6 (60%) on the training subset. Attempts are made to improve this further with the help of other options such as center, scale, pca, cv, etc. Unfortunately no significant improvement is seen and the overall accuracy obtained is ~ 60%
```{r}
fit_rpart_1 <- train(classe~., method="rpart", data=train_xy)
fit_rpart_1
```

```{r, eval=FALSE}
fit_rpart_2 <- train(classe~., method="rpart", data=train_xy,
                     preProcess=c("center","scale"))
fit_rpart_3 <- train(classe~., method="rpart", data=train_xy,
                     preProcess=c("center","scale","pca"))
fit_rpart_4 <- train(classe~., method="rpart", data=train_xy,
                     preProcess=c("center","scale","pca"),trControl=trainControl(method="cv"))
```
The confusion matrix of the predicted vs actual values is shown in the table below. As can be seen, there is a significant mis-classification error rate in the training as well as testing samples.
```{r}
confusionMatrix(predict(fit_rpart_1, train_xy),train_y)
confusionMatrix(predict(fit_rpart_1, test_xy),test_y)
```
### Model=tree
In this section, tree based algorithms are used. With the default options we get a misclassification rate of 0.22 (22%) which implies an accuracy rate of 0.78 (78%). This is definitely an improvement over the previous section. Changing the control parameters does not result in a significant improvement.
```{r}
fit_tree_1 <- tree(classe~.,data=train_xy)
summary(fit_tree_1)
```
The confusion matrix and error rates corresponding to the tree model are shown below. Here too, a significant misclassification error rate is seen in both for in-sample and out of sample cases.
```{r}
confusionMatrix(predict(fit_tree_1, train_xy,type="class"),train_y)
confusionMatrix(predict(fit_tree_1, test_xy,type="class"),test_y)
```
### Model=randomForest
Next the random forest approach is attempted. With default parameters, the misclassification rate is only 0.26%. It is attempted to improve the models further by changing the control parameters such as ```importance```,```proximity```, ```mtry```, etc. Best results are obtained with ```mtry``` of 11
```{r, eval=FALSE}
fit_rf_1 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y)
fit_rf_2 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,importance=TRUE)
fit_rf_3 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,proximity=TRUE)
fit_rf_4 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,mtry=10)
fit_rf_5 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,mtry=11)
fit_rf_6 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,mtry=12)
```
Below are the details of the best achieved fit using Random Forest technique and the confusion matrices on the train, test and validation data subsets. We can see from the confusion matrices that the random forest algorithm does extremely well on all of them. The training sample error rate is 0.2% and the test sample error rate is 0.22%, which is quite good. 
```{r}
fit_rf_5 <- randomForest(x=train_x,y=train_y,xtest=test_x,ytest=test_y,mtry=11)
fit_rf_5
```

```{r,include=FALSE}
fit_rf_7 <- randomForest(x=train_x,y=train_y,xtest=validate,mtry=11)
fit_rf_7
```
Finally we apply the model to the unknown dataset in order to predict the outcome. The results obtained are as below and are submitted and evaluated to be correct.
```{r}
fit_rf_7$test["predicted"]
```

### Summary and Conclusion
In summary, we have developed a model of the WLE dataset by looking at different models and selecting the random Forest model, trained and tested it, and successfully used it for prediction of new observations with excellent accuracy. The specific questions asked in the project related to use of cross validation, explanation of modeling choices and out of sample error rates have also been addressed - to summarize, a 75/25 partitioning scheme was used for cross validation, the explanation of modeling choices made is given in  the section with the same name, and the in sample and out of sample error rates are calculated and shown for each of the models.